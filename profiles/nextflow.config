// sanger-tol job config is here:
// https://github.com/sanger-tol/genomeassembly/blob/dev/conf/base.config

profiles {
    spartan {
        // Spartan limits. There is a bigmem queue with 3 TB available
        params {
            max_cpus = 72
            max_memory = 710000.MB
            max_time = 90.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
            runOptions = '-B $PWD,$TMPDIR,/data --nv -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 710000.MB,
                cpus: 72,
            ]
            executor = 'slurm'
            module = 'Apptainer/1.3.3'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 710000.MB ? 'bigmem' : null }
        }
    }

    pawsey {
        workDir = "${MYSCRATCH}/atol_test_assembly/.nextflow/work"

        params {
            max_cpus = 64
            max_memory = 230.GB
            max_time = 4.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
            runOptions = '-B $PWD,$TMPDIR,/scratch -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 230.GB,
                cpus: 64,
            ]
            executor = 'slurm'
            module = 'singularity/4.1.0-nohost'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 230.GB ? 'highmem' : (task.time > 1.d ? 'long' : null) }
            // Try to avoid the long queue by redifining the time for jobs that
            // request more than 1.d on the first attempt. Subsequent attempts
            // won't be modified. 

            // Pawsey is giving me error 125 when the OOM killer is active. Try
            // to override the default spec (which is here:
            // https://github.com/sanger-tol/genomeassembly/blob/31b508a3bd8998a27f6d06d5dc41bea4707b4a03/conf/base.config#L18)
            errorStrategy = { task.exitStatus in ((130..145) + 104 + 125) ? 'retry' : 'finish' }

            // Reduce the time for HIFIASM to avoid the long queue
            withName: '.*:HIFIASM.*' {
                time = { task.attempt == 1 ? 1.d : null }
            }

            withName: '.*MERQURYFK.*' {
                memory = { task.memory.toMega() * 2 + ' MB' }
            }

            // with s3 I am getting "Failed to create publish directory"
            withName: '.*FASTK.*' {
                publishDir = 'results/sanger_tol/kmer'
            }
        }
        aws {
            client {
                endpoint = 'https://projects.pawsey.org.au'
                s3PathStyleAccess = true
                maxConnections = 1
                maxErrorRetry = 10
                uploadMaxAttempts = 10
            }
        }
    }
}
